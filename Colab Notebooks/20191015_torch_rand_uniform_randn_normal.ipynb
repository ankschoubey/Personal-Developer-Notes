{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"20191015_torch_rand_uniform_randn_normal.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jVjkpaRTDS1L","colab_type":"text"},"source":["Source: https://stackoverflow.com/questions/47240308/differences-between-numpy-random-rand-vs-numpy-random-randn-in-python"]},{"cell_type":"code","metadata":{"id":"yQwC4bJa259Z","colab_type":"code","colab":{}},"source":["import torch\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nngo5Xe228ta","colab_type":"code","colab":{}},"source":["sample_size = 100000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8915TPdH3wDb","colab_type":"code","colab":{}},"source":["#help(torch.rand)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WEcxFzmk3Fue","colab_type":"code","colab":{}},"source":["uniform  = torch.rand(sample_size)\n","pdf, bins, patches = plt.hist(uniform, bins=20, range=(0, 1), density=True) # range is from 0,1 \n","plt.title('rand: uniform')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rvet9pm237ho","colab_type":"code","colab":{}},"source":["# Normal/Gaussian Distribution\n","\n","#help(torch.randn)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIbvYZk9AmkF","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"FGu52xM43H9b","colab_type":"code","colab":{}},"source":["normal = torch.randn(sample_size)\n","pdf, bins, patches = plt.hist(normal, bins=20, range=(-1, 1), density=True) # range is from -1 to 1\n","plt.title('randn: normal')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"cSseYcU83ldE","colab_type":"code","colab":{}},"source":["sample_size = 25"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LqSwEH9HC02R","colab_type":"code","colab":{}},"source":["def sigmoid(x): return 1/(1+torch.exp(-x))\n","# same as\n","torch.sigmoid"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JeWjWv12EeDb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u_5MntMjC8zf","colab_type":"code","colab":{}},"source":["n_nos = torch.arange(sample_size,sample_size)\n","s_nos = sigmoid(n_nos)\n","# https://stackoverflow.com/questions/52922445/runtimeerror-exp-not-implemented-for-torch-longtensor\n","## Use float instead"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qseFuj8HEf5I","colab_type":"code","colab":{}},"source":["n_nos = torch.arange(sample_size * -1.,sample_size)\n","s_nos = sigmoid(n_nos)\n","plt.scatter(range(len(s_nos)),s_nos)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RtCO0uuaFMnN","colab_type":"code","colab":{}},"source":["s_nos = torch.sigmoid(n_nos)\n","plt.scatter(range(len(s_nos)),s_nos)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-3gKtJ4Ffc8","colab_type":"code","colab":{}},"source":["# plotting uniform\n","\n","## less data points are evenly spread through out... which is bad in case of sigmoid because we want our data to be more towards center\n","s_nos = torch.sigmoid(uniform)\n","plt.scatter(uniform,s_nos)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxibPS8bFqVZ","colab_type":"code","colab":{}},"source":["# plotting normal\n","## less data points are at corner... mean is 0\n","s_nos = torch.sigmoid(normal)\n","plt.scatter(normal,s_nos)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yBXzOz5OFpkS","colab_type":"text"},"source":["## From stack overflow mentioned above:\n","\n","So you can see that if your input is away from 0, the slope of the function decreases quite fast and as a result you get a tiny gradient and tiny weight update. \n","\n","And if you have many layers - those gradients get multiplied many times in the back pass, so even \"proper\" gradients after multiplications become small and stop making any influence. \n","\n","**So if you have a lot of weights which bring your input to those regions you network is hardly trainable.**\n","\n","That's why it is a usual practice to **initialize network variables around zero value**. \n","\n","This is done to ensure that you get reasonable gradients (close to 1) to train your net.\n","\n"]},{"cell_type":"code","metadata":{"id":"4OSNhBdy5rUC","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}